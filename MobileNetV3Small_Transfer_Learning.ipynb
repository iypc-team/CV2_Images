{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d145d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11/18/2021-1\n",
    "from __future__ import absolute_import, division\n",
    "from IPython.display import clear_output\n",
    "import glob, os\n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "contentPath = os.getcwd()\n",
    "dataPath = join(contentPath, 'data')\n",
    "testPath = join(contentPath, 'TestImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0e3e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 15:20:00.882228: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-25 15:20:00.882292: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-25 15:20:00.882324: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-iypc-2dteam-2dcv2-5fimages-2dgeh7bvzd): /proc/driver/nvidia/version does not exist\n",
      "2021-11-25 15:20:00.882763: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9221e584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " MobilenetV3small (Functiona  (None, 1, 1, 1024)       1529968   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,530,993\n",
      "Trainable params: 1,025\n",
      "Non-trainable params: 1,529,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.MobileNetV3Small(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be scaled\n",
    "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
    "# outputs: `(inputs * scale) + offset`\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(x)\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49de101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216 files belonging to 2 classes.\n",
      "Using 173 files for training.\n",
      "\n",
      "Found 216 files belonging to 2 classes.\n",
      "Using 43 files for validation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv2Path=join(contentPath, 'CV2Images')\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "train_ds = image_dataset_from_directory(\n",
    "    cv2Path,\n",
    "    color_mode='rgb',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=456,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=4)\n",
    "print()\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    cv2Path,\n",
    "    color_mode='rgb',\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=456,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=4)\n",
    "print()\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe04d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "44/44 [==============================] - 15s 242ms/step - loss: 0.3819 - binary_accuracy: 0.8382 - val_loss: 0.3168 - val_binary_accuracy: 0.9070\n",
      "Epoch 2/3\n",
      "44/44 [==============================] - 9s 207ms/step - loss: 0.3799 - binary_accuracy: 0.8844 - val_loss: 0.3087 - val_binary_accuracy: 0.9070\n",
      "Epoch 3/3\n",
      "44/44 [==============================] - 9s 209ms/step - loss: 0.3875 - binary_accuracy: 0.8844 - val_loss: 0.3107 - val_binary_accuracy: 0.9070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68cc7f1150>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 3\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c7b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " MobilenetV3small (Functiona  (None, 1, 1, 1024)       1529968   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,530,993\n",
      "Trainable params: 1,518,881\n",
      "Non-trainable params: 12,112\n",
      "_________________________________________________________________\n",
      "44/44 [==============================] - 33s 575ms/step - loss: 0.3519 - binary_accuracy: 0.8844 - val_loss: 0.3084 - val_binary_accuracy: 0.9070\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "base_model.trainable = True\n",
    "# model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 1\n",
    "history=model.fit(train_ds,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85af6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MobilenetV3small_retrained.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "modelName = base_model.name + '_retrained.h5'\n",
    "savePath = join(contentPath, modelName)\n",
    "print(savePath)\n",
    "model.save(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004685ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict in module keras.engine.training:\n",
      "\n",
      "predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) method of keras.engine.functional.Functional instance\n",
      "    Generates output predictions for the input samples.\n",
      "    \n",
      "    Computation is done in batches. This method is designed for performance in\n",
      "    large scale inputs. For small amount of inputs that fit in one batch,\n",
      "    directly using `__call__()` is recommended for faster execution, e.g.,\n",
      "    `model(x)`, or `model(x, training=False)` if you have layers such as\n",
      "    `tf.keras.layers.BatchNormalization` that behaves differently during\n",
      "    inference. Also, note the fact that test loss is not affected by\n",
      "    regularization layers like noise and dropout.\n",
      "    \n",
      "    Args:\n",
      "        x: Input samples. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A `tf.data` dataset.\n",
      "          - A generator or `keras.utils.Sequence` instance.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
      "          for iterator-like inputs` section of `Model.fit`.\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per batch.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of dataset, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        verbose: Verbosity mode, 0 or 1.\n",
      "        steps: Total number of steps (batches of samples)\n",
      "            before declaring the prediction round finished.\n",
      "            Ignored with the default value of `None`. If x is a `tf.data`\n",
      "            dataset and `steps` is None, `predict()` will\n",
      "            run until the input dataset is exhausted.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during prediction.\n",
      "            See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up when using\n",
      "            process-based threading. If unspecified, `workers` will default\n",
      "            to 1.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "    \n",
      "    See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
      "    `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
      "    `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
      "    three methods.\n",
      "    \n",
      "    Returns:\n",
      "        Numpy array(s) of predictions.\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: If `model.predict` is wrapped in a `tf.function`.\n",
      "        ValueError: In case of mismatch between the provided\n",
      "            input data and the model's expectations,\n",
      "            or in case a stateful model receives a number of samples\n",
      "            that is not a multiple of the batch size.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model(savePath)\n",
    "# new_model.summary() # Check its architecture\n",
    "inputShape=new_model.input_shape\n",
    "new_model.build(input_shape=inputShape)\n",
    "help(new_model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e9eb96a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/TestImages/TestImages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_902/1625390022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtestGlobList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtestGlobList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/TestImages/TestImages'"
     ]
    }
   ],
   "source": [
    "os.chdir(testPath)\n",
    "testGlobList=[]\n",
    "testGlobList=glob.glob('**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps=224 * 224\n",
    "result=new_model.predict(img,\n",
    "                        batch_size=1,\n",
    "                        verbose=1,\n",
    "                        steps=number_of_steps,\n",
    "                        max_queue_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cadeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
